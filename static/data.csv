dataset,version,metric,mode,qwen2.5-1.5b-instruct-hf
ceval-computer_network,db9ce2,accuracy,gen,68.42
ceval-operating_system,1c2571,accuracy,gen,52.63
ceval-computer_architecture,a74dad,accuracy,gen,76.19
ceval-college_programming,4ca32a,accuracy,gen,70.27
ceval-college_physics,963fa8,accuracy,gen,42.11
ceval-college_chemistry,e78857,accuracy,gen,41.67
ceval-advanced_mathematics,ce03e2,accuracy,gen,21.05
ceval-probability_and_statistics,65e812,accuracy,gen,27.78
ceval-discrete_mathematics,e894ae,accuracy,gen,43.75
ceval-electrical_engineer,ae42b9,accuracy,gen,56.76
ceval-metrology_engineer,ee34ea,accuracy,gen,87.5
ceval-high_school_mathematics,1dc5bf,accuracy,gen,22.22
ceval-high_school_physics,adf25f,accuracy,gen,84.21
ceval-high_school_chemistry,2ed27f,accuracy,gen,52.63
ceval-high_school_biology,8e2b9a,accuracy,gen,68.42
ceval-middle_school_mathematics,bee8d5,accuracy,gen,57.89
ceval-middle_school_biology,86817c,accuracy,gen,90.48
ceval-middle_school_physics,8accf6,accuracy,gen,89.47
ceval-middle_school_chemistry,167a15,accuracy,gen,95
ceval-veterinary_medicine,b4e08d,accuracy,gen,73.91
ceval-college_economics,f3f4e6,accuracy,gen,48
ceval-business_administration,c1614e,accuracy,gen,54.55
ceval-marxism,cf874c,accuracy,gen,78.95
ceval-mao_zedong_thought,51c7a4,accuracy,gen,87.5
ceval-education_science,591fee,accuracy,gen,79.31
ceval-teacher_qualification,4e4ced,accuracy,gen,88.64
ceval-high_school_politics,5c0de2,accuracy,gen,78.95
ceval-high_school_geography,865461,accuracy,gen,73.68
ceval-middle_school_politics,5be3e7,accuracy,gen,85.71
ceval-middle_school_geography,8a63be,accuracy,gen,91.67
ceval-modern_chinese_history,fc01af,accuracy,gen,86.96
ceval-ideological_and_moral_cultivation,a2aa4a,accuracy,gen,100
ceval-logic,f5b022,accuracy,gen,59.09
ceval-law,a110a1,accuracy,gen,45.83
ceval-chinese_language_and_literature,0f8b68,accuracy,gen,47.83
ceval-art_studies,2a1300,accuracy,gen,63.64
ceval-professional_tour_guide,4e673e,accuracy,gen,79.31
ceval-legal_professional,ce8787,accuracy,gen,56.52
ceval-high_school_chinese,315705,accuracy,gen,36.84
ceval-high_school_history,7eb30a,accuracy,gen,70
ceval-middle_school_history,48ab4a,accuracy,gen,90.91
ceval-civil_servant,87d061,accuracy,gen,57.45
ceval-sports_science,70f27b,accuracy,gen,68.42
ceval-plant_protection,8941f9,accuracy,gen,59.09
ceval-basic_medicine,c409d6,accuracy,gen,73.68
ceval-clinical_medicine,49e82d,accuracy,gen,59.09
ceval-urban_and_rural_planner,95b885,accuracy,gen,63.04
ceval-accountant,2837,accuracy,gen,65.31
ceval-fire_engineer,bc23f5,accuracy,gen,67.74
ceval-environmental_impact_assessment_engineer,c64e2d,accuracy,gen,64.52
ceval-tax_accountant,3a5e3c,accuracy,gen,61.22
ceval-physician,6e277d,accuracy,gen,71.43
WiC,d06864,accuracy,gen,54
afqmc-dev,901306,accuracy,gen,66
GaokaoBench_2010-2022_Math_II_MCQs,5b0b29,score,gen,66
GaokaoBench_2010-2022_Math_I_MCQs,5b0b29,score,gen,56
GaokaoBench_2010-2022_History_MCQs,3613b5,score,gen,58
GaokaoBench_2010-2022_Biology_MCQs,d26e80,score,gen,62
GaokaoBench_2010-2022_Political_Science_MCQs,70fce9,score,gen,78
GaokaoBench_2010-2022_Physics_MCQs,8a0c30,score,gen,25
GaokaoBench_2010-2022_Chemistry_MCQs,852bbd,score,gen,24
GaokaoBench_2010-2013_English_MCQs,01b50f,score,gen,68
GaokaoBench_2010-2022_Chinese_Modern_Lit,e19c31,score,gen,41.38
GaokaoBench_2010-2022_English_Fill_in_Blanks,924021,score,gen,39.67
GaokaoBench_2012-2022_English_Cloze_Test,11f6ce,score,gen,19.23
GaokaoBench_2010-2022_Geography_MCQs,862192,score,gen,55.79
GaokaoBench_2010-2022_English_Reading_Comp,ffdef4,score,gen,61.66
GaokaoBench_2010-2022_Chinese_Lang_and_Usage_MCQs,ba10b2,score,gen,32.26
GaokaoBench_2010-2022_Math_I_Fill-in-the-Blank,9dd6c7,score,gen,0
GaokaoBench_2010-2022_Math_II_Fill-in-the-Blank,9dd6c7,score,gen,0
GaokaoBench_2010-2022_Chinese_Language_Famous_Passages_and_Sentences_Dictation,58053d,score,gen,0
GaokaoBench_2014-2022_English_Language_Cloze_Passage,d431f7,score,gen,0
GaokaoBench_2010-2022_Geography_Open-ended_Questions,475d13,score,gen,0
GaokaoBench_2010-2022_Chemistry_Open-ended_Questions,0bccc3,score,gen,0
GaokaoBench_2010-2022_Math_I_Open-ended_Questions,8a0cf5,score,gen,0
GaokaoBench_2010-2022_History_Open-ended_Questions,9fbad8,score,gen,0
GaokaoBench_2010-2022_Biology_Open-ended_Questions,e0cb29,score,gen,0
GaokaoBench_2010-2022_Math_II_Open-ended_Questions,8a0cf5,score,gen,0
GaokaoBench_2010-2022_Physics_Open-ended_Questions,423d19,score,gen,0
GaokaoBench_2010-2022_Political_Science_Open-ended_Questions,0eee0a,score,gen,0
GaokaoBench_2012-2022_English_Language_Error_Correction,6a3cae,score,gen,0
GaokaoBench_2010-2022_Chinese_Language_Ancient_Poetry_Reading,ee6cc7,score,gen,0
GaokaoBench_2010-2022_Chinese_Language_Practical_Text_Reading,4dea5a,score,gen,0
GaokaoBench_2010-2022_Chinese_Language_Literary_Text_Reading,979d8b,score,gen,0
GaokaoBench_2010-2022_Chinese_Language_Classical_Chinese_Reading,9de717,score,gen,0
GaokaoBench_2010-2022_Chinese_Language_Language_and_Writing_Skills_Open-ended_Questions,d2ed84,score,gen,0
tydiqa-goldp_arabic,3d688e,f1,gen,38.17
tydiqa-goldp_arabic,3d688e,exact_match,gen,6
tydiqa-goldp_bengali,ed3379,f1,gen,21.23
tydiqa-goldp_bengali,ed3379,exact_match,gen,0
tydiqa-goldp_english,38da40,f1,gen,12.69
tydiqa-goldp_english,38da40,exact_match,gen,0
tydiqa-goldp_finnish,4adb40,f1,gen,17.63
tydiqa-goldp_finnish,4adb40,exact_match,gen,0
tydiqa-goldp_indonesian,7b2a2a,f1,gen,22.71
tydiqa-goldp_indonesian,7b2a2a,exact_match,gen,8
tydiqa-goldp_japanese,8e7cb9,f1,gen,2
tydiqa-goldp_japanese,8e7cb9,exact_match,gen,2
tydiqa-goldp_korean,5be5b3,f1,gen,23.62
tydiqa-goldp_korean,5be5b3,exact_match,gen,12
tydiqa-goldp_russian,4070c2,f1,gen,18.9
tydiqa-goldp_russian,4070c2,exact_match,gen,4
tydiqa-goldp_swahili,97110f,f1,gen,6.35
tydiqa-goldp_swahili,97110f,exact_match,gen,0
tydiqa-goldp_telugu,df948c,f1,gen,2.2
tydiqa-goldp_telugu,df948c,exact_match,gen,0
tydiqa-goldp_thai,3e6b28,f1,gen,30.41
tydiqa-goldp_thai,3e6b28,exact_match,gen,0
triviaqa,2121ce,score,gen,42
nq,c788f6,score,gen,18
C3,8c358f,accuracy,gen,80
race-middle,9a54b6,accuracy,gen,88
race-high,9a54b6,accuracy,gen,82
lcsts,8ee1fe,rouge1,gen,17.98
lcsts,8ee1fe,rouge2,gen,5.83
lcsts,8ee1fe,rougeL,gen,15.36
Xsum,31397e,rouge1,gen,22.11
Xsum,31397e,rouge2,gen,4.66
Xsum,31397e,rougeL,gen,14.41
Xsum,31397e,rougeLsum,gen,14.44
lambada,2.17E+13,accuracy,gen,32
COPA,91ca53,accuracy,gen,70
ReCoRD,30dea0,score,gen,28
math,393424,accuracy,gen,40
gsm8k,1d7fe4,accuracy,gen,62
drop,8a9ed9,score,gen,48
mbpp,1e1056,score,gen,42
mbpp,1e1056,pass,gen,21
mbpp,1e1056,timeout,gen,0
mbpp,1e1056,failed,gen,7
mbpp,1e1056,wrong_answer,gen,22
bbh-temporal_sequences,e43931,score,gen,20
bbh-disambiguation_qa,d52c61,score,gen,36
bbh-date_understanding,a8000b,score,gen,36
bbh-tracking_shuffled_objects_three_objects,7964c0,score,gen,36
bbh-penguins_in_a_table,fceb27,score,gen,54
bbh-geometric_shapes,503c8f,score,gen,8
bbh-snarks,42d6ca,score,gen,62
bbh-ruin_names,408de8,score,gen,34
bbh-tracking_shuffled_objects_seven_objects,7964c0,score,gen,12
bbh-tracking_shuffled_objects_five_objects,7964c0,score,gen,18
bbh-logical_deduction_three_objects,45ebc5,score,gen,50
bbh-hyperbaton,5e5016,score,gen,76
bbh-logical_deduction_five_objects,45ebc5,score,gen,20
bbh-logical_deduction_seven_objects,45ebc5,score,gen,12
bbh-movie_recommendation,cc2fde,score,gen,50
bbh-salient_translation_error_detection,5b5f35,score,gen,46
bbh-reasoning_about_colored_objects,1cb761,score,gen,62
bbh-multistep_arithmetic_two,30f91e,score,gen,60
bbh-navigate,1576d9,score,gen,70
bbh-dyck_languages,805bea,score,gen,0
bbh-word_sorting,9a3f78,score,gen,2
bbh-sports_understanding,d3fa77,score,gen,42
bbh-boolean_expressions,612c92,score,gen,86
bbh-object_counting,781e5c,score,gen,54
bbh-formal_fallacies,eada96,score,gen,4
bbh-causal_judgement,89eaa4,score,gen,52
bbh-web_of_lies,0c0441,score,gen,70
lukaemon_mmlu_college_biology,caec7d,accuracy,gen,72
lukaemon_mmlu_college_chemistry,520aa6,accuracy,gen,42
lukaemon_mmlu_college_computer_science,99c216,accuracy,gen,56
lukaemon_mmlu_college_mathematics,678751,accuracy,gen,44
lukaemon_mmlu_college_physics,4f382c,accuracy,gen,44
lukaemon_mmlu_electrical_engineering,770ce3,accuracy,gen,66
lukaemon_mmlu_astronomy,d3ee01,accuracy,gen,74
lukaemon_mmlu_anatomy,72183b,accuracy,gen,50
lukaemon_mmlu_abstract_algebra,2db373,accuracy,gen,38
lukaemon_mmlu_machine_learning,0283bb,accuracy,gen,48
lukaemon_mmlu_clinical_knowledge,cb3218,accuracy,gen,68
lukaemon_mmlu_global_facts,ab07b6,accuracy,gen,24
lukaemon_mmlu_management,80876d,accuracy,gen,82
lukaemon_mmlu_nutrition,4543bd,accuracy,gen,68
lukaemon_mmlu_marketing,7.39E+06,accuracy,gen,90
lukaemon_mmlu_professional_accounting,444b7f,accuracy,gen,46
lukaemon_mmlu_high_school_geography,7.80E+08,accuracy,gen,74
lukaemon_mmlu_international_law,cf3179,accuracy,gen,78
lukaemon_mmlu_moral_scenarios,f6dbe2,accuracy,gen,32
lukaemon_mmlu_computer_security,ce7550,accuracy,gen,78
lukaemon_mmlu_high_school_microeconomics,04d21a,accuracy,gen,70
lukaemon_mmlu_professional_law,5f7e6c,accuracy,gen,44
lukaemon_mmlu_medical_genetics,881ef5,accuracy,gen,70
lukaemon_mmlu_professional_psychology,221a16,accuracy,gen,72
lukaemon_mmlu_jurisprudence,001f24,accuracy,gen,76
lukaemon_mmlu_world_religions,232c09,accuracy,gen,82
lukaemon_mmlu_philosophy,08042b,accuracy,gen,68
lukaemon_mmlu_virology,1.20E+271,accuracy,gen,48
lukaemon_mmlu_high_school_chemistry,ae8820,accuracy,gen,60
lukaemon_mmlu_public_relations,e7d39b,accuracy,gen,62
lukaemon_mmlu_high_school_macroeconomics,a01685,accuracy,gen,70
lukaemon_mmlu_human_sexuality,42407c,accuracy,gen,74
lukaemon_mmlu_elementary_mathematics,269926,accuracy,gen,48
lukaemon_mmlu_high_school_physics,93278f,accuracy,gen,36
lukaemon_mmlu_high_school_computer_science,9965a5,accuracy,gen,68
lukaemon_mmlu_high_school_european_history,eefc90,accuracy,gen,72
lukaemon_mmlu_business_ethics,1-Dec-08,accuracy,gen,70
lukaemon_mmlu_moral_disputes,a2173e,accuracy,gen,62
lukaemon_mmlu_high_school_statistics,8f3f3a,accuracy,gen,44
lukaemon_mmlu_miscellaneous,935647,accuracy,gen,66
lukaemon_mmlu_formal_logic,cfcb0c,accuracy,gen,52
lukaemon_mmlu_high_school_government_and_politics,3c52f9,accuracy,gen,88
lukaemon_mmlu_prehistory,bbb197,accuracy,gen,66
lukaemon_mmlu_security_studies,9b1743,accuracy,gen,64
lukaemon_mmlu_high_school_biology,37b125,accuracy,gen,76
lukaemon_mmlu_logical_fallacies,9cebb0,accuracy,gen,76
lukaemon_mmlu_high_school_world_history,048e7e,accuracy,gen,76
lukaemon_mmlu_professional_medicine,857144,accuracy,gen,52
lukaemon_mmlu_high_school_mathematics,ed4dc0,accuracy,gen,28
lukaemon_mmlu_college_medicine,38709e,accuracy,gen,70
lukaemon_mmlu_high_school_us_history,8932df,accuracy,gen,66
lukaemon_mmlu_sociology,c266a2,accuracy,gen,68
lukaemon_mmlu_econometrics,d1134d,accuracy,gen,36
lukaemon_mmlu_high_school_psychology,7db114,accuracy,gen,80
lukaemon_mmlu_human_aging,82a410,accuracy,gen,68
lukaemon_mmlu_us_foreign_policy,528cfe,accuracy,gen,78
lukaemon_mmlu_conceptual_physics,63588e,accuracy,gen,76
cmmlu-agronomy,4c7f2c,accuracy,gen,44
cmmlu-anatomy,ea09bf,accuracy,gen,52
cmmlu-ancient_chinese,f7c97f,accuracy,gen,38
cmmlu-arts,dd77b8,accuracy,gen,96
cmmlu-astronomy,1e49db,accuracy,gen,50
cmmlu-business_ethics,dc78cb,accuracy,gen,64
cmmlu-chinese_civil_service_exam,1de82c,accuracy,gen,54
cmmlu-chinese_driving_rule,b8a42b,accuracy,gen,88
cmmlu-chinese_food_culture,2d568a,accuracy,gen,52
cmmlu-chinese_foreign_policy,dc2427,accuracy,gen,64
cmmlu-chinese_history,4cc7ed,accuracy,gen,66
cmmlu-chinese_literature,af3c41,accuracy,gen,52
cmmlu-chinese_teacher_qualification,87de11,accuracy,gen,88
cmmlu-clinical_knowledge,c55b1d,accuracy,gen,50
cmmlu-college_actuarial_science,d3c360,accuracy,gen,26
cmmlu-college_education,df8790,accuracy,gen,76
cmmlu-college_engineering_hydrology,673f23,accuracy,gen,54
cmmlu-college_law,524c3a,accuracy,gen,48
cmmlu-college_mathematics,e4ebad,accuracy,gen,44
cmmlu-college_medical_statistics,55af35,accuracy,gen,54
cmmlu-college_medicine,702f48,accuracy,gen,72
cmmlu-computer_science,637007,accuracy,gen,82
cmmlu-computer_security,932b6b,accuracy,gen,84
cmmlu-conceptual_physics,cfc077,accuracy,gen,74
cmmlu-construction_project_management,968a4a,accuracy,gen,58
cmmlu-economics,ddaf7c,accuracy,gen,80
cmmlu-education,c35963,accuracy,gen,68
cmmlu-electrical_engineering,70e98a,accuracy,gen,60
cmmlu-elementary_chinese,cbcd6a,accuracy,gen,70
cmmlu-elementary_commonsense,a67f37,accuracy,gen,78
cmmlu-elementary_information_and_technology,d34d2a,accuracy,gen,84
cmmlu-elementary_mathematics,a9d403,accuracy,gen,46
cmmlu-ethnology,31955f,accuracy,gen,66
cmmlu-food_science,741d8e,accuracy,gen,52
cmmlu-genetics,c326f7,accuracy,gen,46
cmmlu-global_facts,0a1236,accuracy,gen,62
cmmlu-high_school_biology,2be811,accuracy,gen,62
cmmlu-high_school_chemistry,d63c05,accuracy,gen,46
cmmlu-high_school_geography,5cd489,accuracy,gen,66
cmmlu-high_school_mathematics,6b2087,accuracy,gen,46
cmmlu-high_school_physics,3df353,accuracy,gen,52
cmmlu-high_school_politics,7a88d8,accuracy,gen,62
cmmlu-human_sexuality,54ac98,accuracy,gen,58
cmmlu-international_law,0f5d40,accuracy,gen,38
cmmlu-journalism,a4f6a0,accuracy,gen,56
cmmlu-jurisprudence,7843da,accuracy,gen,68
cmmlu-legal_and_moral_basis,f906b0,accuracy,gen,96
cmmlu-logical,15a71b,accuracy,gen,50
cmmlu-machine_learning,bc6ad4,accuracy,gen,56
cmmlu-management,e5e8db,accuracy,gen,70
cmmlu-marketing,8b4c18,accuracy,gen,62
cmmlu-marxist_theory,75eb79,accuracy,gen,94
cmmlu-modern_chinese,83a9b7,accuracy,gen,42
cmmlu-nutrition,adfff7,accuracy,gen,72
cmmlu-philosophy,75e22d,accuracy,gen,72
cmmlu-professional_accounting,0edc91,accuracy,gen,76
cmmlu-professional_law,d24af5,accuracy,gen,58
cmmlu-professional_medicine,134139,accuracy,gen,66
cmmlu-professional_psychology,ec920e,accuracy,gen,78
cmmlu-public_relations,70ee06,accuracy,gen,54
cmmlu-security_study,45f96f,accuracy,gen,72
cmmlu-sociology,485285,accuracy,gen,64
cmmlu-sports_science,838cfe,accuracy,gen,66
cmmlu-traditional_chinese_medicine,3bbf64,accuracy,gen,70
cmmlu-virology,8925bf,accuracy,gen,64
cmmlu-world_history,57c97c,accuracy,gen,66
cmmlu-world_religions,1d0f4b,accuracy,gen,70
